{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import lightning as L\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from timm import create_model\n",
    "from qat.export.utils import replace_module_by_name, fetch_module_by_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.jit import Final\n",
    "# from timm.layers import use_fused_attn\n",
    "# class Attention2(nn.Module):\n",
    "#     fused_attn: Final[bool]\n",
    "#     def __init__(\n",
    "#             self,\n",
    "#             dim,\n",
    "#             num_heads=8,\n",
    "#             qkv_bias=False,\n",
    "#             qk_norm=False,\n",
    "#             attn_drop=0.,\n",
    "#             proj_drop=0.,\n",
    "#             norm_layer=nn.LayerNorm,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n",
    "#         self.num_heads = num_heads\n",
    "#         self.head_dim = dim // num_heads\n",
    "#         self.scale = self.head_dim ** -0.5\n",
    "#         self.fused_attn = use_fused_attn()\n",
    "\n",
    "#         # self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "#         self.q_linear = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "#         self.k_linear = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "#         self.v_linear = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "\n",
    "#         self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n",
    "#         self.k_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n",
    "#         self.attn_drop = nn.Dropout(attn_drop)\n",
    "#         self.proj = nn.Linear(dim, dim)\n",
    "#         self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         B, N, C = x.shape\n",
    "#         # qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4) # (3, B, heads, N, d)\n",
    "#         # q, k, v = qkv.unbind(0)\n",
    "#         q = self.q_linear(x).reshape(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "#         k = self.k_linear(x).reshape(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "#         v = self.v_linear(x).reshape(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "#         q, k = self.q_norm(q), self.k_norm(k)\n",
    "\n",
    "#         if self.fused_attn:\n",
    "#             x = F.scaled_dot_product_attention(\n",
    "#                 q, k, v,\n",
    "#                 dropout_p=self.attn_drop.p,\n",
    "#             )\n",
    "#         else:\n",
    "#             q = q * self.scale\n",
    "#             attn = q @ k.transpose(-2, -1)\n",
    "#             attn = attn.softmax(dim=-1)\n",
    "#             attn = self.attn_drop(attn)\n",
    "#             x = attn @ v\n",
    "\n",
    "#         x = x.transpose(1, 2).reshape(B, N, C)\n",
    "#         x = self.proj(x)\n",
    "#         x = self.proj_drop(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.jit import Final\n",
    "from timm.layers import use_fused_attn\n",
    "class Attention3(nn.Module):\n",
    "    fused_attn: Final[bool]\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim,\n",
    "            num_heads=8,\n",
    "            qkv_bias=False,\n",
    "            qk_norm=False,\n",
    "            attn_drop=0.,\n",
    "            proj_drop=0.,\n",
    "            norm_layer=nn.LayerNorm,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.fused_attn = use_fused_attn()\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n",
    "        self.k_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        self.register_parameter(\n",
    "            \"inverse_temperature_logit\",\n",
    "            nn.Parameter(torch.randn(1))\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)\n",
    "        q, k = self.q_norm(q), self.k_norm(k)\n",
    "\n",
    "        if self.fused_attn:\n",
    "            x = F.scaled_dot_product_attention(\n",
    "                q, k, v,\n",
    "                dropout_p=self.attn_drop.p,\n",
    "            )\n",
    "        else:\n",
    "            q = q * self.scale\n",
    "            attn = q @ k.transpose(-2, -1)\n",
    "            attn = attn.softmax(dim=-1)\n",
    "            attn_argmax = attn.argmax(dim=-1)\n",
    "            attn = self.attn_drop(attn)\n",
    "            output = attn - (attn - attn_argmax).detach() # output = real_output - (real_output - quantized_output).detach()\n",
    "            x = output @ v\n",
    "\n",
    "        x = x.transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"deit3_huge_patch14_224.fb_in22k_ft_in1k\"\n",
    "# model_name = \"deit3_base_patch16_224.fb_in22k_ft_in1k\"\n",
    "model_name = 'deit3_small_patch16_224.fb_in22k_ft_in1k'\n",
    "model = create_model(model_name, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "VisionTransformer                        [1, 1000]                 75,648\n",
       "├─PatchEmbed: 1-1                        [1, 196, 384]             --\n",
       "│    └─Conv2d: 2-1                       [1, 384, 14, 14]          295,296\n",
       "│    └─Identity: 2-2                     [1, 196, 384]             --\n",
       "├─Dropout: 1-2                           [1, 197, 384]             --\n",
       "├─Identity: 1-3                          [1, 197, 384]             --\n",
       "├─Identity: 1-4                          [1, 197, 384]             --\n",
       "├─Sequential: 1-5                        [1, 197, 384]             --\n",
       "│    └─Block: 2-3                        [1, 197, 384]             --\n",
       "│    │    └─LayerNorm: 3-1               [1, 197, 384]             768\n",
       "│    │    └─Attention: 3-2               [1, 197, 384]             591,360\n",
       "│    │    └─LayerScale: 3-3              [1, 197, 384]             384\n",
       "│    │    └─Identity: 3-4                [1, 197, 384]             --\n",
       "│    │    └─LayerNorm: 3-5               [1, 197, 384]             768\n",
       "│    │    └─Mlp: 3-6                     [1, 197, 384]             1,181,568\n",
       "│    │    └─LayerScale: 3-7              [1, 197, 384]             384\n",
       "│    │    └─Identity: 3-8                [1, 197, 384]             --\n",
       "│    └─Block: 2-4                        [1, 197, 384]             --\n",
       "│    │    └─LayerNorm: 3-9               [1, 197, 384]             768\n",
       "│    │    └─Attention: 3-10              [1, 197, 384]             591,360\n",
       "│    │    └─LayerScale: 3-11             [1, 197, 384]             384\n",
       "│    │    └─Identity: 3-12               [1, 197, 384]             --\n",
       "│    │    └─LayerNorm: 3-13              [1, 197, 384]             768\n",
       "│    │    └─Mlp: 3-14                    [1, 197, 384]             1,181,568\n",
       "│    │    └─LayerScale: 3-15             [1, 197, 384]             384\n",
       "│    │    └─Identity: 3-16               [1, 197, 384]             --\n",
       "│    └─Block: 2-5                        [1, 197, 384]             --\n",
       "│    │    └─LayerNorm: 3-17              [1, 197, 384]             768\n",
       "│    │    └─Attention: 3-18              [1, 197, 384]             591,360\n",
       "│    │    └─LayerScale: 3-19             [1, 197, 384]             384\n",
       "│    │    └─Identity: 3-20               [1, 197, 384]             --\n",
       "│    │    └─LayerNorm: 3-21              [1, 197, 384]             768\n",
       "│    │    └─Mlp: 3-22                    [1, 197, 384]             1,181,568\n",
       "│    │    └─LayerScale: 3-23             [1, 197, 384]             384\n",
       "│    │    └─Identity: 3-24               [1, 197, 384]             --\n",
       "│    └─Block: 2-6                        [1, 197, 384]             --\n",
       "│    │    └─LayerNorm: 3-25              [1, 197, 384]             768\n",
       "│    │    └─Attention: 3-26              [1, 197, 384]             591,360\n",
       "│    │    └─LayerScale: 3-27             [1, 197, 384]             384\n",
       "│    │    └─Identity: 3-28               [1, 197, 384]             --\n",
       "│    │    └─LayerNorm: 3-29              [1, 197, 384]             768\n",
       "│    │    └─Mlp: 3-30                    [1, 197, 384]             1,181,568\n",
       "│    │    └─LayerScale: 3-31             [1, 197, 384]             384\n",
       "│    │    └─Identity: 3-32               [1, 197, 384]             --\n",
       "│    └─Block: 2-7                        [1, 197, 384]             --\n",
       "│    │    └─LayerNorm: 3-33              [1, 197, 384]             768\n",
       "│    │    └─Attention: 3-34              [1, 197, 384]             591,360\n",
       "│    │    └─LayerScale: 3-35             [1, 197, 384]             384\n",
       "│    │    └─Identity: 3-36               [1, 197, 384]             --\n",
       "│    │    └─LayerNorm: 3-37              [1, 197, 384]             768\n",
       "│    │    └─Mlp: 3-38                    [1, 197, 384]             1,181,568\n",
       "│    │    └─LayerScale: 3-39             [1, 197, 384]             384\n",
       "│    │    └─Identity: 3-40               [1, 197, 384]             --\n",
       "│    └─Block: 2-8                        [1, 197, 384]             --\n",
       "│    │    └─LayerNorm: 3-41              [1, 197, 384]             768\n",
       "│    │    └─Attention: 3-42              [1, 197, 384]             591,360\n",
       "│    │    └─LayerScale: 3-43             [1, 197, 384]             384\n",
       "│    │    └─Identity: 3-44               [1, 197, 384]             --\n",
       "│    │    └─LayerNorm: 3-45              [1, 197, 384]             768\n",
       "│    │    └─Mlp: 3-46                    [1, 197, 384]             1,181,568\n",
       "│    │    └─LayerScale: 3-47             [1, 197, 384]             384\n",
       "│    │    └─Identity: 3-48               [1, 197, 384]             --\n",
       "│    └─Block: 2-9                        [1, 197, 384]             --\n",
       "│    │    └─LayerNorm: 3-49              [1, 197, 384]             768\n",
       "│    │    └─Attention: 3-50              [1, 197, 384]             591,360\n",
       "│    │    └─LayerScale: 3-51             [1, 197, 384]             384\n",
       "│    │    └─Identity: 3-52               [1, 197, 384]             --\n",
       "│    │    └─LayerNorm: 3-53              [1, 197, 384]             768\n",
       "│    │    └─Mlp: 3-54                    [1, 197, 384]             1,181,568\n",
       "│    │    └─LayerScale: 3-55             [1, 197, 384]             384\n",
       "│    │    └─Identity: 3-56               [1, 197, 384]             --\n",
       "│    └─Block: 2-10                       [1, 197, 384]             --\n",
       "│    │    └─LayerNorm: 3-57              [1, 197, 384]             768\n",
       "│    │    └─Attention: 3-58              [1, 197, 384]             591,360\n",
       "│    │    └─LayerScale: 3-59             [1, 197, 384]             384\n",
       "│    │    └─Identity: 3-60               [1, 197, 384]             --\n",
       "│    │    └─LayerNorm: 3-61              [1, 197, 384]             768\n",
       "│    │    └─Mlp: 3-62                    [1, 197, 384]             1,181,568\n",
       "│    │    └─LayerScale: 3-63             [1, 197, 384]             384\n",
       "│    │    └─Identity: 3-64               [1, 197, 384]             --\n",
       "│    └─Block: 2-11                       [1, 197, 384]             --\n",
       "│    │    └─LayerNorm: 3-65              [1, 197, 384]             768\n",
       "│    │    └─Attention: 3-66              [1, 197, 384]             591,360\n",
       "│    │    └─LayerScale: 3-67             [1, 197, 384]             384\n",
       "│    │    └─Identity: 3-68               [1, 197, 384]             --\n",
       "│    │    └─LayerNorm: 3-69              [1, 197, 384]             768\n",
       "│    │    └─Mlp: 3-70                    [1, 197, 384]             1,181,568\n",
       "│    │    └─LayerScale: 3-71             [1, 197, 384]             384\n",
       "│    │    └─Identity: 3-72               [1, 197, 384]             --\n",
       "│    └─Block: 2-12                       [1, 197, 384]             --\n",
       "│    │    └─LayerNorm: 3-73              [1, 197, 384]             768\n",
       "│    │    └─Attention: 3-74              [1, 197, 384]             591,360\n",
       "│    │    └─LayerScale: 3-75             [1, 197, 384]             384\n",
       "│    │    └─Identity: 3-76               [1, 197, 384]             --\n",
       "│    │    └─LayerNorm: 3-77              [1, 197, 384]             768\n",
       "│    │    └─Mlp: 3-78                    [1, 197, 384]             1,181,568\n",
       "│    │    └─LayerScale: 3-79             [1, 197, 384]             384\n",
       "│    │    └─Identity: 3-80               [1, 197, 384]             --\n",
       "│    └─Block: 2-13                       [1, 197, 384]             --\n",
       "│    │    └─LayerNorm: 3-81              [1, 197, 384]             768\n",
       "│    │    └─Attention: 3-82              [1, 197, 384]             591,360\n",
       "│    │    └─LayerScale: 3-83             [1, 197, 384]             384\n",
       "│    │    └─Identity: 3-84               [1, 197, 384]             --\n",
       "│    │    └─LayerNorm: 3-85              [1, 197, 384]             768\n",
       "│    │    └─Mlp: 3-86                    [1, 197, 384]             1,181,568\n",
       "│    │    └─LayerScale: 3-87             [1, 197, 384]             384\n",
       "│    │    └─Identity: 3-88               [1, 197, 384]             --\n",
       "│    └─Block: 2-14                       [1, 197, 384]             --\n",
       "│    │    └─LayerNorm: 3-89              [1, 197, 384]             768\n",
       "│    │    └─Attention: 3-90              [1, 197, 384]             591,360\n",
       "│    │    └─LayerScale: 3-91             [1, 197, 384]             384\n",
       "│    │    └─Identity: 3-92               [1, 197, 384]             --\n",
       "│    │    └─LayerNorm: 3-93              [1, 197, 384]             768\n",
       "│    │    └─Mlp: 3-94                    [1, 197, 384]             1,181,568\n",
       "│    │    └─LayerScale: 3-95             [1, 197, 384]             384\n",
       "│    │    └─Identity: 3-96               [1, 197, 384]             --\n",
       "├─LayerNorm: 1-6                         [1, 197, 384]             768\n",
       "├─Identity: 1-7                          [1, 384]                  --\n",
       "├─Dropout: 1-8                           [1, 384]                  --\n",
       "├─Linear: 1-9                            [1, 1000]                 385,000\n",
       "==========================================================================================\n",
       "Total params: 22,059,496\n",
       "Trainable params: 22,059,496\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 79.56\n",
       "==========================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 95.62\n",
       "Params size (MB): 87.94\n",
       "Estimated Total Size (MB): 184.16\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model1, (1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (patch_drop): Identity()\n",
       "  (norm_pre): Identity()\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "  (fc_norm): Identity()\n",
       "  (head_drop): Dropout(p=0.0, inplace=False)\n",
       "  (head): Linear(in_features=384, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings = {\n",
    "#     \"attn\": 384,\n",
    "# }\n",
    "\n",
    "# for i in range(0, 12): \n",
    "#     for name in settings:\n",
    "#         layer = model.blocks[i]\n",
    "#         module = fetch_module_by_name(layer, name)\n",
    "#         attn = Attention3(\n",
    "#                 dim=settings[name], # 為什麼從attn module拿不到 head_dim?\n",
    "#                 num_heads=getattr(module, \"num_heads\"),\n",
    "#                 qkv_bias=True,\n",
    "#                 qk_norm=False\n",
    "#             )\n",
    "        \n",
    "#         qkv_weight = module.qkv.weight.data\n",
    "#         qkv_bias = module.qkv.bias.data\n",
    "#         dim = qkv_weight.shape[0] // 3\n",
    "#         q_weight, k_weight, v_weight = qkv_weight[:dim], qkv_weight[dim:2*dim], qkv_weight[2*dim:]\n",
    "#         # print(q_weight.shape)\n",
    "#         q_bias, k_bias, v_bias = qkv_bias[:dim], qkv_bias[dim:2*dim], qkv_bias[2*dim:]\n",
    "#         attn.q_linear.weight.data = q_weight\n",
    "#         attn.q_linear.bias.data = q_bias\n",
    "#         attn.k_linear.weight.data = k_weight\n",
    "#         attn.k_linear.bias.data = k_bias\n",
    "#         attn.v_linear.weight.data = v_weight\n",
    "#         attn.v_linear.bias.data = v_bias\n",
    "#         attn.attn_drop = module.attn_drop\n",
    "#         attn.proj = module.proj\n",
    "#         attn.proj_drop = module.proj_drop\n",
    "#         replace_module_by_name(layer, name, attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    \"attn\": 384, # dimension of the model\n",
    "}\n",
    "\n",
    "for i in range(0, 12): \n",
    "    for name in settings:\n",
    "        layer = model.blocks[i]\n",
    "        module = fetch_module_by_name(layer, name)\n",
    "        attn = Attention3(\n",
    "                dim=settings[name],\n",
    "                num_heads=getattr(module, \"num_heads\"),\n",
    "                qkv_bias=True,\n",
    "                qk_norm=False\n",
    "            )\n",
    "        attn.inverse_temperature_logit.data.copy_(\n",
    "                torch.tensor(10)\n",
    "            )\n",
    "        qkv_weight = module.qkv.weight.data\n",
    "        qkv_bias = module.qkv.bias.data\n",
    "        attn.qkv.weight.data = qkv_weight\n",
    "        attn.qkv.bias.data = qkv_bias\n",
    "        attn.attn_drop = module.attn_drop\n",
    "        attn.proj = module.proj\n",
    "        attn.proj_drop = module.proj_drop\n",
    "        replace_module_by_name(layer, name, attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([10.], requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.blocks[0].attn.inverse_temperature_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, f'{model_name}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['resmlp_12_224', 'resmlp_24_224', 'resmlp_36_224', 'resmlp_big_24_224']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timm\n",
    "timm.list_models(\"resmlp*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "timm.list_models(\"resmlp*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResBlock(\n",
       "  (norm1): Affine()\n",
       "  (linear_tokens): Linear(in_features=196, out_features=196, bias=True)\n",
       "  (drop_path): Identity()\n",
       "  (norm2): Affine()\n",
       "  (mlp_channels): Mlp(\n",
       "    (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "    (act): GELU(approximate='none')\n",
       "    (drop1): Dropout(p=0.0, inplace=False)\n",
       "    (norm): Identity()\n",
       "    (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "    (drop2): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.blocks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The GCD of 196 and 384 is 4\n"
     ]
    }
   ],
   "source": [
    "def gcd(a, b):\n",
    "    while b:\n",
    "        a, b = b, a % b\n",
    "    return abs(a)\n",
    "\n",
    "# 測試\n",
    "x = 196\n",
    "y = 384\n",
    "print(f\"The GCD of {x} and {y} is {gcd(x, y)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "from timm import create_model\n",
    "model_name = 'resmlp_12_224.fb_in1k'\n",
    "model = create_model(model_name, pretrained=True)\n",
    "summary(model,(1,3,224,224)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MlpMixer(\n",
       "  (stem): PatchEmbed(\n",
       "    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (blocks): Sequential(\n",
       "    (0): ResBlock(\n",
       "      (norm1): Affine()\n",
       "      (linear_tokens): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (drop_path): Identity()\n",
       "      (norm2): Affine()\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): ResBlock(\n",
       "      (norm1): Affine()\n",
       "      (linear_tokens): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (drop_path): Identity()\n",
       "      (norm2): Affine()\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): ResBlock(\n",
       "      (norm1): Affine()\n",
       "      (linear_tokens): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (drop_path): Identity()\n",
       "      (norm2): Affine()\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): ResBlock(\n",
       "      (norm1): Affine()\n",
       "      (linear_tokens): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (drop_path): Identity()\n",
       "      (norm2): Affine()\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): ResBlock(\n",
       "      (norm1): Affine()\n",
       "      (linear_tokens): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (drop_path): Identity()\n",
       "      (norm2): Affine()\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): ResBlock(\n",
       "      (norm1): Affine()\n",
       "      (linear_tokens): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (drop_path): Identity()\n",
       "      (norm2): Affine()\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): ResBlock(\n",
       "      (norm1): Affine()\n",
       "      (linear_tokens): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (drop_path): Identity()\n",
       "      (norm2): Affine()\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): ResBlock(\n",
       "      (norm1): Affine()\n",
       "      (linear_tokens): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (drop_path): Identity()\n",
       "      (norm2): Affine()\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): ResBlock(\n",
       "      (norm1): Affine()\n",
       "      (linear_tokens): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (drop_path): Identity()\n",
       "      (norm2): Affine()\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): ResBlock(\n",
       "      (norm1): Affine()\n",
       "      (linear_tokens): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (drop_path): Identity()\n",
       "      (norm2): Affine()\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): ResBlock(\n",
       "      (norm1): Affine()\n",
       "      (linear_tokens): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (drop_path): Identity()\n",
       "      (norm2): Affine()\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): ResBlock(\n",
       "      (norm1): Affine()\n",
       "      (linear_tokens): Linear(in_features=196, out_features=196, bias=True)\n",
       "      (drop_path): Identity()\n",
       "      (norm2): Affine()\n",
       "      (mlp_channels): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): Affine()\n",
       "  (head_drop): Dropout(p=0.0, inplace=False)\n",
       "  (head): Linear(in_features=384, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "model_name = 'deit3_huge_patch14_224.fb_in22k_ft_in1k'\n",
    "model = create_model(model_name, pretrained=True)\n",
    "summary(model,(1,3,224,224)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
