{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import lightning as L\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from timm import create_model\n",
    "from qat.export.utils import replace_module_by_name, fetch_module_by_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.jit import Final\n",
    "from timm.layers import use_fused_attn\n",
    "class Attention2(nn.Module):\n",
    "    fused_attn: Final[bool]\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim,\n",
    "            num_heads=8,\n",
    "            qkv_bias=False,\n",
    "            qk_norm=False,\n",
    "            attn_drop=0.,\n",
    "            proj_drop=0.,\n",
    "            norm_layer=nn.LayerNorm,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.fused_attn = use_fused_attn()\n",
    "\n",
    "        # self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.q_linear = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.k_linear = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.v_linear = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "\n",
    "        self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n",
    "        self.k_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        # qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4) # (3, B, heads, N, d)\n",
    "        # q, k, v = qkv.unbind(0)\n",
    "        q = self.q_linear(x).reshape(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        k = self.k_linear(x).reshape(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        v = self.v_linear(x).reshape(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        q, k = self.q_norm(q), self.k_norm(k)\n",
    "\n",
    "        if self.fused_attn:\n",
    "            x = F.scaled_dot_product_attention(\n",
    "                q, k, v,\n",
    "                dropout_p=self.attn_drop.p,\n",
    "            )\n",
    "        else:\n",
    "            q = q * self.scale\n",
    "            attn = q @ k.transpose(-2, -1)\n",
    "            attn = attn.softmax(dim=-1)\n",
    "            attn = self.attn_drop(attn)\n",
    "            x = attn @ v\n",
    "\n",
    "        x = x.transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.jit import Final\n",
    "# from timm.layers import use_fused_attn\n",
    "# class Attention3(nn.Module):\n",
    "#     fused_attn: Final[bool]\n",
    "#     def __init__(\n",
    "#             self,\n",
    "#             dim,\n",
    "#             num_heads=8,\n",
    "#             qkv_bias=False,\n",
    "#             qk_norm=False,\n",
    "#             attn_drop=0.,\n",
    "#             proj_drop=0.,\n",
    "#             norm_layer=nn.LayerNorm,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n",
    "#         self.num_heads = num_heads\n",
    "#         self.head_dim = dim // num_heads\n",
    "#         self.scale = self.head_dim ** -0.5\n",
    "#         self.fused_attn = use_fused_attn()\n",
    "\n",
    "#         self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "#         self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n",
    "#         self.k_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n",
    "#         self.attn_drop = nn.Dropout(attn_drop)\n",
    "#         self.proj = nn.Linear(dim, dim)\n",
    "#         self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         B, N, C = x.shape\n",
    "#         qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "#         q, k, v = qkv.unbind(0)\n",
    "#         q, k = self.q_norm(q), self.k_norm(k)\n",
    "\n",
    "#         if self.fused_attn:\n",
    "#             x = F.scaled_dot_product_attention(\n",
    "#                 q, k, v,\n",
    "#                 dropout_p=self.attn_drop.p,\n",
    "#             )\n",
    "#         else:\n",
    "#             q = q * self.scale\n",
    "#             attn = q @ k.transpose(-2, -1)\n",
    "#             attn = attn.softmax(dim=-1)\n",
    "#             attn_argmax = attn.argmax(dim=-1)\n",
    "#             attn = self.attn_drop(attn)\n",
    "#             output = attn - (attn - attn_argmax).detach() # output = real_output - (real_output - quantized_output).detach()\n",
    "#             x = output @ v\n",
    "\n",
    "#         x = x.transpose(1, 2).reshape(B, N, C)\n",
    "#         x = self.proj(x)\n",
    "#         x = self.proj_drop(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"deit3_huge_patch14_224.fb_in22k_ft_in1k\"\n",
    "# model_name = \"deit3_base_patch16_224.fb_in22k_ft_in1k\"\n",
    "model_name = 'deit3_small_patch16_224.fb_in22k_ft_in1k'\n",
    "model = create_model(model_name, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    \"attn\": 384,\n",
    "}\n",
    "\n",
    "for i in range(0, 12): \n",
    "    for name in settings:\n",
    "        layer = model.blocks[i]\n",
    "        module = fetch_module_by_name(layer, name)\n",
    "        attn = Attention2(\n",
    "                dim=settings[name], # 為什麼從attn module拿不到 head_dim?\n",
    "                num_heads=getattr(module, \"num_heads\"),\n",
    "                qkv_bias=True,\n",
    "                qk_norm=False\n",
    "            )\n",
    "        \n",
    "        qkv_weight = module.qkv.weight.data\n",
    "        qkv_bias = module.qkv.bias.data\n",
    "        dim = qkv_weight.shape[0] // 3\n",
    "        q_weight, k_weight, v_weight = qkv_weight[:dim], qkv_weight[dim:2*dim], qkv_weight[2*dim:]\n",
    "        # print(q_weight.shape)\n",
    "        q_bias, k_bias, v_bias = qkv_bias[:dim], qkv_bias[dim:2*dim], qkv_bias[2*dim:]\n",
    "        attn.q_linear.weight.data = q_weight\n",
    "        attn.q_linear.bias.data = q_bias\n",
    "        attn.k_linear.weight.data = k_weight\n",
    "        attn.k_linear.bias.data = k_bias\n",
    "        attn.v_linear.weight.data = v_weight\n",
    "        attn.v_linear.bias.data = v_bias\n",
    "        attn.attn_drop = module.attn_drop\n",
    "        attn.proj = module.proj\n",
    "        attn.proj_drop = module.proj_drop\n",
    "        replace_module_by_name(layer, name, attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings = {\n",
    "#     \"attn\": 1280, # dimension of the model\n",
    "# }\n",
    "\n",
    "# for i in range(0, 31): \n",
    "#     for name in settings:\n",
    "#         layer = model.blocks[i]\n",
    "#         module = fetch_module_by_name(layer, name)\n",
    "#         attn = Attention3(\n",
    "#                 dim=settings[name],\n",
    "#                 num_heads=getattr(module, \"num_heads\"),\n",
    "#                 qkv_bias=True,\n",
    "#                 qk_norm=False\n",
    "#             )\n",
    "#         qkv_weight = module.qkv.weight.data\n",
    "#         qkv_bias = module.qkv.bias.data\n",
    "#         attn.qkv.weight.data = qkv_weight\n",
    "#         attn.qkv.bias.data = qkv_bias\n",
    "#         attn.attn_drop = module.attn_drop\n",
    "#         attn.proj = module.proj\n",
    "#         attn.proj_drop = module.proj_drop\n",
    "#         replace_module_by_name(layer, name, attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, f'{model_name}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "timm.list_models(\"deit3*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "model_name = 'deit3_huge_patch14_224.fb_in22k_ft_in1k'\n",
    "model = create_model(model_name, pretrained=True)\n",
    "summary(model,(1,3,224,224)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
