{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import lightning as L\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from timm import create_model\n",
    "from qat.export.utils import replace_module_by_name, fetch_module_by_name\n",
    "from networks.vision_transformer import VisionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnScore(nn.Module):\n",
    "    def __init__(self, scale, attn_drop):\n",
    "        super().__init__()\n",
    "        self.scale = scale\n",
    "        self.attn_drop = nn.Dropout(attn_drop) # one is func. another one is probability.\n",
    "    def forward(self, q, k):\n",
    "        q = q * self.scale\n",
    "        attn = q @ k.transpose(-2, -1)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        return attn\n",
    "\n",
    "class Attention2(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim,\n",
    "            num_heads=8,\n",
    "            qkv_bias=False,\n",
    "            qk_norm=False,\n",
    "            attn_drop=0.,\n",
    "            proj_drop=0.,\n",
    "            norm_layer=nn.LayerNorm,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        # self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.q_linear = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.k_linear = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.v_linear = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.attn_score = AttnScore(scale=self.scale, attn_drop=attn_drop)\n",
    "        self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n",
    "        self.k_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        # qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        # q, k, v = qkv.unbind(0)\n",
    "        q = self.q_linear(x).reshape(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        k = self.k_linear(x).reshape(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        v = self.v_linear(x).reshape(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        q, k = self.q_norm(q), self.k_norm(k)\n",
    "\n",
    "        # if self.fused_attn:\n",
    "        #     x = F.scaled_dot_product_attention(\n",
    "        #         q, k, v,\n",
    "        #         dropout_p=self.attn_drop.p,\n",
    "        #     )\n",
    "        # else:\n",
    "        #     # q = q * self.scale\n",
    "        #     # attn = q @ k.transpose(-2, -1)\n",
    "        #     # attn = attn.softmax(dim=-1)\n",
    "        #     # attn = self.attn_drop(attn)\n",
    "        #     attn = self.attn_score(q, k)\n",
    "        #     x = attn @ v\n",
    "        attn = self.attn_score(q, k)\n",
    "        x = attn @ v\n",
    "        x = x.transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.jit import Final\n",
    "# from timm.layers import use_fused_attn\n",
    "# class Attention3(nn.Module):\n",
    "#     fused_attn: Final[bool]\n",
    "#     def __init__(\n",
    "#             self,\n",
    "#             dim,\n",
    "#             num_heads=8,\n",
    "#             qkv_bias=False,\n",
    "#             qk_norm=False,\n",
    "#             attn_drop=0.,\n",
    "#             proj_drop=0.,\n",
    "#             norm_layer=nn.LayerNorm,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n",
    "#         self.num_heads = num_heads\n",
    "#         self.head_dim = dim // num_heads\n",
    "#         self.scale = self.head_dim ** -0.5\n",
    "#         self.fused_attn = use_fused_attn()\n",
    "\n",
    "#         self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "#         self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n",
    "#         self.k_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n",
    "#         self.attn_drop = nn.Dropout(attn_drop)\n",
    "#         self.proj = nn.Linear(dim, dim)\n",
    "#         self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         B, N, C = x.shape\n",
    "#         qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "#         q, k, v = qkv.unbind(0)\n",
    "#         q, k = self.q_norm(q), self.k_norm(k)\n",
    "\n",
    "#         if self.fused_attn:\n",
    "#             x = F.scaled_dot_product_attention(\n",
    "#                 q, k, v,\n",
    "#                 dropout_p=self.attn_drop.p,\n",
    "#             )\n",
    "#         else:\n",
    "#             q = q * self.scale\n",
    "#             attn = q @ k.transpose(-2, -1)\n",
    "#             attn = attn.softmax(dim=-1)\n",
    "#             attn_argmax = attn.argmax(dim=-1)\n",
    "#             attn = self.attn_drop(attn)\n",
    "#             output = attn - (attn - attn_argmax).detach() # output = real_output - (real_output - quantized_output).detach()\n",
    "#             x = output @ v\n",
    "\n",
    "#         x = x.transpose(1, 2).reshape(B, N, C)\n",
    "#         x = self.proj(x)\n",
    "#         x = self.proj_drop(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"deit3_huge_patch14_224.fb_in22k_ft_in1k\"\n",
    "# model_name = \"deit3_base_patch16_224.fb_in22k_ft_in1k\"\n",
    "model_name = 'deit3_small_patch16_224.fb_in22k_ft_in1k'\n",
    "model = create_model(model_name, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    \"attn\": 384,\n",
    "}\n",
    "\n",
    "for i in range(0, 12): \n",
    "    for name in settings:\n",
    "        layer = model.blocks[i]\n",
    "        module = fetch_module_by_name(layer, name)\n",
    "        attn = Attention2(\n",
    "                dim=settings[name], # 為什麼從attn module拿不到 head_dim?\n",
    "                num_heads=getattr(module, \"num_heads\"),\n",
    "                qkv_bias=True,\n",
    "                qk_norm=False\n",
    "            )\n",
    "        \n",
    "        qkv_weight = module.qkv.weight.data\n",
    "        qkv_bias = module.qkv.bias.data\n",
    "        dim = qkv_weight.shape[0] // 3\n",
    "        q_weight, k_weight, v_weight = qkv_weight[:dim], qkv_weight[dim:2*dim], qkv_weight[2*dim:]\n",
    "        # print(q_weight.shape)\n",
    "        q_bias, k_bias, v_bias = qkv_bias[:dim], qkv_bias[dim:2*dim], qkv_bias[2*dim:]\n",
    "        attn.q_linear.weight.data = q_weight\n",
    "        attn.q_linear.bias.data = q_bias\n",
    "        attn.k_linear.weight.data = k_weight\n",
    "        attn.k_linear.bias.data = k_bias\n",
    "        attn.v_linear.weight.data = v_weight\n",
    "        attn.v_linear.bias.data = v_bias\n",
    "        attn.attn_drop = module.attn_drop\n",
    "        attn.proj = module.proj\n",
    "        attn.proj_drop = module.proj_drop\n",
    "        replace_module_by_name(layer, name, attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (patch_drop): Identity()\n",
       "  (norm_pre): Identity()\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention2(\n",
       "        (q_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (k_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (v_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (attn_score): AttnScore(\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention2(\n",
       "        (q_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (k_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (v_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (attn_score): AttnScore(\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention2(\n",
       "        (q_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (k_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (v_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (attn_score): AttnScore(\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention2(\n",
       "        (q_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (k_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (v_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (attn_score): AttnScore(\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention2(\n",
       "        (q_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (k_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (v_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (attn_score): AttnScore(\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention2(\n",
       "        (q_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (k_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (v_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (attn_score): AttnScore(\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention2(\n",
       "        (q_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (k_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (v_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (attn_score): AttnScore(\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention2(\n",
       "        (q_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (k_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (v_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (attn_score): AttnScore(\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention2(\n",
       "        (q_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (k_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (v_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (attn_score): AttnScore(\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention2(\n",
       "        (q_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (k_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (v_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (attn_score): AttnScore(\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention2(\n",
       "        (q_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (k_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (v_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (attn_score): AttnScore(\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention2(\n",
       "        (q_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (k_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (v_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (attn_score): AttnScore(\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "  (fc_norm): Identity()\n",
       "  (head_drop): Dropout(p=0.0, inplace=False)\n",
       "  (head): Linear(in_features=384, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_tensors = []\n",
    "# def hook(module, input_tensor, output_tensor): # The hook will be called every time after :func:`forward` has computed an output.\n",
    "#     tensor = output_tensor.cpu()\n",
    "#     output_tensors.append(tensor)\n",
    "    \n",
    "# handles = []\n",
    "# for name, module in model.named_modules(): \n",
    "#     if \"attn_score\" in name: # double check module_name\n",
    "#         handles.append(module.register_forward_hook(hook))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "model_0 = deepcopy(model)\n",
    "for i in range(1, 12):\n",
    "    model_0.blocks[i] = nn.Identity()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (patch_drop): Identity()\n",
       "  (norm_pre): Identity()\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention2(\n",
       "        (q_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (k_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (v_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (attn_score): AttnScore(\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (1): Identity()\n",
       "    (2): Identity()\n",
       "    (3): Identity()\n",
       "    (4): Identity()\n",
       "    (5): Identity()\n",
       "    (6): Identity()\n",
       "    (7): Identity()\n",
       "    (8): Identity()\n",
       "    (9): Identity()\n",
       "    (10): Identity()\n",
       "    (11): Identity()\n",
       "  )\n",
       "  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "  (fc_norm): Identity()\n",
       "  (head_drop): Dropout(p=0.0, inplace=False)\n",
       "  (head): Linear(in_features=384, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 3, 224, 224).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0.norm = nn.Identity()\n",
    "model_0.head = nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 197, 384])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "VisionTransformer                        [1, 197, 1000]            75,648\n",
       "├─PatchEmbed: 1-1                        [1, 196, 384]             --\n",
       "│    └─Conv2d: 2-1                       [1, 384, 14, 14]          295,296\n",
       "│    └─Identity: 2-2                     [1, 196, 384]             --\n",
       "├─Dropout: 1-2                           [1, 197, 384]             --\n",
       "├─Identity: 1-3                          [1, 197, 384]             --\n",
       "├─Identity: 1-4                          [1, 197, 384]             --\n",
       "├─Sequential: 1-5                        [1, 197, 384]             --\n",
       "│    └─Block: 2-3                        [1, 197, 384]             --\n",
       "│    │    └─LayerNorm: 3-1               [1, 197, 384]             768\n",
       "│    │    └─Attention2: 3-2              [1, 197, 384]             591,360\n",
       "│    │    └─LayerScale: 3-3              [1, 197, 384]             384\n",
       "│    │    └─Identity: 3-4                [1, 197, 384]             --\n",
       "│    │    └─LayerNorm: 3-5               [1, 197, 384]             768\n",
       "│    │    └─Mlp: 3-6                     [1, 197, 384]             1,181,568\n",
       "│    │    └─LayerScale: 3-7              [1, 197, 384]             384\n",
       "│    │    └─Identity: 3-8                [1, 197, 384]             --\n",
       "│    └─Block: 2-4                        [1, 197, 384]             --\n",
       "│    │    └─LayerNorm: 3-9               [1, 197, 384]             768\n",
       "│    │    └─Attention2: 3-10             [1, 197, 384]             591,360\n",
       "│    │    └─LayerScale: 3-11             [1, 197, 384]             384\n",
       "│    │    └─Identity: 3-12               [1, 197, 384]             --\n",
       "│    │    └─LayerNorm: 3-13              [1, 197, 384]             768\n",
       "│    │    └─Mlp: 3-14                    [1, 197, 384]             1,181,568\n",
       "│    │    └─LayerScale: 3-15             [1, 197, 384]             384\n",
       "│    │    └─Identity: 3-16               [1, 197, 384]             --\n",
       "│    └─Block: 2-5                        [1, 197, 384]             --\n",
       "│    │    └─LayerNorm: 3-17              [1, 197, 384]             768\n",
       "│    │    └─Attention2: 3-18             [1, 197, 384]             591,360\n",
       "│    │    └─LayerScale: 3-19             [1, 197, 384]             384\n",
       "│    │    └─Identity: 3-20               [1, 197, 384]             --\n",
       "│    │    └─LayerNorm: 3-21              [1, 197, 384]             768\n",
       "│    │    └─Mlp: 3-22                    [1, 197, 384]             1,181,568\n",
       "│    │    └─LayerScale: 3-23             [1, 197, 384]             384\n",
       "│    │    └─Identity: 3-24               [1, 197, 384]             --\n",
       "│    └─Block: 2-6                        [1, 197, 384]             --\n",
       "│    │    └─LayerNorm: 3-25              [1, 197, 384]             768\n",
       "│    │    └─Attention2: 3-26             [1, 197, 384]             591,360\n",
       "│    │    └─LayerScale: 3-27             [1, 197, 384]             384\n",
       "│    │    └─Identity: 3-28               [1, 197, 384]             --\n",
       "│    │    └─LayerNorm: 3-29              [1, 197, 384]             768\n",
       "│    │    └─Mlp: 3-30                    [1, 197, 384]             1,181,568\n",
       "│    │    └─LayerScale: 3-31             [1, 197, 384]             384\n",
       "│    │    └─Identity: 3-32               [1, 197, 384]             --\n",
       "│    └─Block: 2-7                        [1, 197, 384]             --\n",
       "│    │    └─LayerNorm: 3-33              [1, 197, 384]             768\n",
       "│    │    └─Attention2: 3-34             [1, 197, 384]             591,360\n",
       "│    │    └─LayerScale: 3-35             [1, 197, 384]             384\n",
       "│    │    └─Identity: 3-36               [1, 197, 384]             --\n",
       "│    │    └─LayerNorm: 3-37              [1, 197, 384]             768\n",
       "│    │    └─Mlp: 3-38                    [1, 197, 384]             1,181,568\n",
       "│    │    └─LayerScale: 3-39             [1, 197, 384]             384\n",
       "│    │    └─Identity: 3-40               [1, 197, 384]             --\n",
       "│    └─Block: 2-8                        [1, 197, 384]             --\n",
       "│    │    └─LayerNorm: 3-41              [1, 197, 384]             768\n",
       "│    │    └─Attention2: 3-42             [1, 197, 384]             591,360\n",
       "│    │    └─LayerScale: 3-43             [1, 197, 384]             384\n",
       "│    │    └─Identity: 3-44               [1, 197, 384]             --\n",
       "│    │    └─LayerNorm: 3-45              [1, 197, 384]             768\n",
       "│    │    └─Mlp: 3-46                    [1, 197, 384]             1,181,568\n",
       "│    │    └─LayerScale: 3-47             [1, 197, 384]             384\n",
       "│    │    └─Identity: 3-48               [1, 197, 384]             --\n",
       "│    └─Block: 2-9                        [1, 197, 384]             --\n",
       "│    │    └─LayerNorm: 3-49              [1, 197, 384]             768\n",
       "│    │    └─Attention2: 3-50             [1, 197, 384]             591,360\n",
       "│    │    └─LayerScale: 3-51             [1, 197, 384]             384\n",
       "│    │    └─Identity: 3-52               [1, 197, 384]             --\n",
       "│    │    └─LayerNorm: 3-53              [1, 197, 384]             768\n",
       "│    │    └─Mlp: 3-54                    [1, 197, 384]             1,181,568\n",
       "│    │    └─LayerScale: 3-55             [1, 197, 384]             384\n",
       "│    │    └─Identity: 3-56               [1, 197, 384]             --\n",
       "│    └─Block: 2-10                       [1, 197, 384]             --\n",
       "│    │    └─LayerNorm: 3-57              [1, 197, 384]             768\n",
       "│    │    └─Attention2: 3-58             [1, 197, 384]             591,360\n",
       "│    │    └─LayerScale: 3-59             [1, 197, 384]             384\n",
       "│    │    └─Identity: 3-60               [1, 197, 384]             --\n",
       "│    │    └─LayerNorm: 3-61              [1, 197, 384]             768\n",
       "│    │    └─Mlp: 3-62                    [1, 197, 384]             1,181,568\n",
       "│    │    └─LayerScale: 3-63             [1, 197, 384]             384\n",
       "│    │    └─Identity: 3-64               [1, 197, 384]             --\n",
       "│    └─Block: 2-11                       [1, 197, 384]             --\n",
       "│    │    └─LayerNorm: 3-65              [1, 197, 384]             768\n",
       "│    │    └─Attention2: 3-66             [1, 197, 384]             591,360\n",
       "│    │    └─LayerScale: 3-67             [1, 197, 384]             384\n",
       "│    │    └─Identity: 3-68               [1, 197, 384]             --\n",
       "│    │    └─LayerNorm: 3-69              [1, 197, 384]             768\n",
       "│    │    └─Mlp: 3-70                    [1, 197, 384]             1,181,568\n",
       "│    │    └─LayerScale: 3-71             [1, 197, 384]             384\n",
       "│    │    └─Identity: 3-72               [1, 197, 384]             --\n",
       "│    └─Block: 2-12                       [1, 197, 384]             --\n",
       "│    │    └─LayerNorm: 3-73              [1, 197, 384]             768\n",
       "│    │    └─Attention2: 3-74             [1, 197, 384]             591,360\n",
       "│    │    └─LayerScale: 3-75             [1, 197, 384]             384\n",
       "│    │    └─Identity: 3-76               [1, 197, 384]             --\n",
       "│    │    └─LayerNorm: 3-77              [1, 197, 384]             768\n",
       "│    │    └─Mlp: 3-78                    [1, 197, 384]             1,181,568\n",
       "│    │    └─LayerScale: 3-79             [1, 197, 384]             384\n",
       "│    │    └─Identity: 3-80               [1, 197, 384]             --\n",
       "│    └─Block: 2-13                       [1, 197, 384]             --\n",
       "│    │    └─LayerNorm: 3-81              [1, 197, 384]             768\n",
       "│    │    └─Attention2: 3-82             [1, 197, 384]             591,360\n",
       "│    │    └─LayerScale: 3-83             [1, 197, 384]             384\n",
       "│    │    └─Identity: 3-84               [1, 197, 384]             --\n",
       "│    │    └─LayerNorm: 3-85              [1, 197, 384]             768\n",
       "│    │    └─Mlp: 3-86                    [1, 197, 384]             1,181,568\n",
       "│    │    └─LayerScale: 3-87             [1, 197, 384]             384\n",
       "│    │    └─Identity: 3-88               [1, 197, 384]             --\n",
       "│    └─Block: 2-14                       [1, 197, 384]             --\n",
       "│    │    └─LayerNorm: 3-89              [1, 197, 384]             768\n",
       "│    │    └─Attention2: 3-90             [1, 197, 384]             591,360\n",
       "│    │    └─LayerScale: 3-91             [1, 197, 384]             384\n",
       "│    │    └─Identity: 3-92               [1, 197, 384]             --\n",
       "│    │    └─LayerNorm: 3-93              [1, 197, 384]             768\n",
       "│    │    └─Mlp: 3-94                    [1, 197, 384]             1,181,568\n",
       "│    │    └─LayerScale: 3-95             [1, 197, 384]             384\n",
       "│    │    └─Identity: 3-96               [1, 197, 384]             --\n",
       "├─LayerNorm: 1-6                         [1, 197, 384]             768\n",
       "├─Identity: 1-7                          [1, 197, 384]             --\n",
       "├─Dropout: 1-8                           [1, 197, 384]             --\n",
       "├─Linear: 1-9                            [1, 197, 1000]            385,000\n",
       "==========================================================================================\n",
       "Total params: 22,059,496\n",
       "Trainable params: 22,059,496\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 79.56\n",
       "==========================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 97.19\n",
       "Params size (MB): 87.94\n",
       "Estimated Total Size (MB): 185.73\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model,(1, 3, 224, 224)) \n",
    "# for handle in handles:\n",
    "    # handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(output_tensors)\n",
    "# output_tensors[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings = {\n",
    "#     \"attn\": 1280, # dimension of the model\n",
    "# }\n",
    "\n",
    "# for i in range(0, 31): \n",
    "#     for name in settings:\n",
    "#         layer = model.blocks[i]\n",
    "#         module = fetch_module_by_name(layer, name)\n",
    "#         attn = Attention3(\n",
    "#                 dim=settings[name],\n",
    "#                 num_heads=getattr(module, \"num_heads\"),\n",
    "#                 qkv_bias=True,\n",
    "#                 qk_norm=False\n",
    "#             )\n",
    "#         qkv_weight = module.qkv.weight.data\n",
    "#         qkv_bias = module.qkv.bias.data\n",
    "#         attn.qkv.weight.data = qkv_weight\n",
    "#         attn.qkv.bias.data = qkv_bias\n",
    "#         attn.attn_drop = module.attn_drop\n",
    "#         attn.proj = module.proj\n",
    "#         attn.proj_drop = module.proj_drop\n",
    "#         replace_module_by_name(layer, name, attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (patch_drop): Identity()\n",
       "  (norm_pre): Identity()\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention2(\n",
       "        (q_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (k_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (v_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (attn_score): AttnScore(\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention2(\n",
       "        (q_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (k_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (v_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (attn_score): AttnScore(\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention2(\n",
       "        (q_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (k_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (v_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (attn_score): AttnScore(\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention2(\n",
       "        (q_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (k_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (v_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (attn_score): AttnScore(\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention2(\n",
       "        (q_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (k_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (v_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (attn_score): AttnScore(\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention2(\n",
       "        (q_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (k_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (v_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (attn_score): AttnScore(\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention2(\n",
       "        (q_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (k_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (v_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (attn_score): AttnScore(\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention2(\n",
       "        (q_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (k_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (v_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (attn_score): AttnScore(\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention2(\n",
       "        (q_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (k_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (v_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (attn_score): AttnScore(\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention2(\n",
       "        (q_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (k_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (v_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (attn_score): AttnScore(\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention2(\n",
       "        (q_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (k_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (v_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (attn_score): AttnScore(\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention2(\n",
       "        (q_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (k_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (v_linear): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (attn_score): AttnScore(\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "  (fc_norm): Identity()\n",
       "  (head_drop): Dropout(p=0.0, inplace=False)\n",
       "  (head): Linear(in_features=384, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'deit3_small_patch16_224.fb_in22k_ft_in1k'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name # deit3_small_patch16_224.fb_in22k_ft_in1k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'{model_name}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['deit3_base_patch16_224',\n",
       " 'deit3_base_patch16_384',\n",
       " 'deit3_huge_patch14_224',\n",
       " 'deit3_large_patch16_224',\n",
       " 'deit3_large_patch16_384',\n",
       " 'deit3_medium_patch16_224',\n",
       " 'deit3_small_patch16_224',\n",
       " 'deit3_small_patch16_384']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timm\n",
    "timm.list_models(\"deit3*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchinfo import summary\n",
    "# model_name = 'deit3_huge_patch14_224.fb_in22k_ft_in1k'\n",
    "# model = create_model(model_name, pretrained=True)\n",
    "# summary(model,(1,3,224,224)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
